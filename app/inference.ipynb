{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "465ce445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusion.model.builder import build_model, get_tokenizer_and_text_encoder, get_vae, vae_decode\n",
    "from diffusion.model.utils import get_weight_dtype, prepare_prompt_ar, resize_and_crop_tensor\n",
    "from diffusion.utils.config import SanaConfig, model_init_config\n",
    "from diffusion.utils.logger import get_root_logger\n",
    "\n",
    "from diffusion import DPMS_SDE\n",
    "from app.sana_pipeline_tts import SanaPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698dcfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 13:38:45 - \u001b[1m[Sana]\u001b[0m - INFO - Sampler flow_dpm-solver, flow_shift: 3.0\n",
      "2025-08-04 13:38:45 - \u001b[1m[Sana]\u001b[0m - INFO - Inference with torch.bfloat16, PAG guidance layer: [8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m[AutoencoderDC] Loading model from mit-han-lab/dc-ae-f32c32-sana-1.1-diffusers\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:4')\n",
    "config = \"../configs/sana_config/1024ms/Sana_1600M_img1024.yaml\"\n",
    "model_path = \"hf://Efficient-Large-Model/Sana_1600M_1024px_BF16/checkpoints/Sana_1600M_1024px_BF16.pth\"\n",
    "pipe = SanaPipeline(config, device)\n",
    "pipe.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dd7f369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guidance_type_select(default_guidance_type, pag_scale, attn_type):\n",
    "    guidance_type = default_guidance_type\n",
    "    if not (pag_scale > 1.0 and attn_type == \"linear\"):\n",
    "        guidance_type = \"classifier-free\"\n",
    "    elif pag_scale > 1.0 and attn_type == \"linear\":\n",
    "        guidance_type = \"classifier-free_PAG\"\n",
    "    return guidance_type\n",
    "\n",
    "\n",
    "def classify_height_width_bin(height: int, width: int, ratios: dict):\n",
    "    \"\"\"Returns binned height and width.\"\"\"\n",
    "    ar = float(height / width)\n",
    "    closest_ratio = min(ratios.keys(), key=lambda ratio: abs(float(ratio) - ar))\n",
    "    default_hw = ratios[closest_ratio]\n",
    "    return int(default_hw[0]), int(default_hw[1])\n",
    "\n",
    "@torch.inference_mode()\n",
    "def forward(\n",
    "    self,\n",
    "    prompt=None,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    negative_prompt=\"\",\n",
    "    num_inference_steps=20,\n",
    "    guidance_scale=4.5,\n",
    "    pag_guidance_scale=1.0,\n",
    "    num_images_per_prompt=1,\n",
    "    generator=torch.Generator().manual_seed(42),\n",
    "    latents=None,\n",
    "    use_resolution_binning=True,\n",
    "):\n",
    "    self.ori_height, self.ori_width = height, width\n",
    "    if use_resolution_binning:\n",
    "        self.height, self.width = classify_height_width_bin(height, width, ratios=self.base_ratios)\n",
    "    else:\n",
    "        self.height, self.width = height, width\n",
    "    self.latent_size_h, self.latent_size_w = (\n",
    "        self.height // self.config.vae.vae_downsample_rate,\n",
    "        self.width // self.config.vae.vae_downsample_rate,\n",
    "    )\n",
    "    self.guidance_type = guidance_type_select(self.guidance_type, pag_guidance_scale, self.config.model.attn_type)\n",
    "\n",
    "    # 1. pre-compute negative embedding\n",
    "    if negative_prompt != \"\":\n",
    "        null_caption_token = self.tokenizer(\n",
    "            negative_prompt,\n",
    "            max_length=self.max_sequence_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(self.device)\n",
    "        self.null_caption_embs = self.text_encoder(null_caption_token.input_ids, null_caption_token.attention_mask)[\n",
    "            0\n",
    "        ]\n",
    "\n",
    "    if prompt is None:\n",
    "        prompt = [\"\"]\n",
    "    prompts = prompt if isinstance(prompt, list) else [prompt]\n",
    "    samples = []\n",
    "\n",
    "    for prompt in prompts:\n",
    "        # data prepare\n",
    "        prompts, hw, ar = (\n",
    "            [],\n",
    "            torch.tensor([[self.image_size, self.image_size]], dtype=torch.float, device=self.device).repeat(\n",
    "                num_images_per_prompt, 1\n",
    "            ),\n",
    "            torch.tensor([[1.0]], device=self.device).repeat(num_images_per_prompt, 1),\n",
    "        )\n",
    "\n",
    "        for _ in range(num_images_per_prompt):\n",
    "            prompts.append(prepare_prompt_ar(prompt, self.base_ratios, device=self.device, show=False)[0].strip())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # prepare text feature\n",
    "            if not self.config.text_encoder.chi_prompt:\n",
    "                max_length_all = self.config.text_encoder.model_max_length\n",
    "                prompts_all = prompts\n",
    "            else:\n",
    "                chi_prompt = \"\\n\".join(self.config.text_encoder.chi_prompt)\n",
    "                prompts_all = [chi_prompt + prompt for prompt in prompts]\n",
    "                num_chi_prompt_tokens = len(self.tokenizer.encode(chi_prompt))\n",
    "                max_length_all = (\n",
    "                    num_chi_prompt_tokens + self.config.text_encoder.model_max_length - 2\n",
    "                )  # magic number 2: [bos], [_]\n",
    "\n",
    "            caption_token = self.tokenizer(\n",
    "                prompts_all,\n",
    "                max_length=max_length_all,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(device=self.device)\n",
    "            select_index = [0] + list(range(-self.config.text_encoder.model_max_length + 1, 0))\n",
    "            caption_embs = self.text_encoder(caption_token.input_ids, caption_token.attention_mask)[0][:, None][\n",
    "                :, :, select_index\n",
    "            ].to(self.weight_dtype)\n",
    "            emb_masks = caption_token.attention_mask[:, select_index]\n",
    "            null_y = self.null_caption_embs.repeat(len(prompts), 1, 1)[:, None].to(self.weight_dtype)\n",
    "\n",
    "            n = len(prompts)\n",
    "            if latents is None:\n",
    "                z = torch.randn(\n",
    "                    n,\n",
    "                    self.config.vae.vae_latent_dim,\n",
    "                    self.latent_size_h,\n",
    "                    self.latent_size_w,\n",
    "                    generator=generator,\n",
    "                    device=self.device,\n",
    "                    # dtype=self.weight_dtype,\n",
    "                )\n",
    "            else:\n",
    "                z = latents.to(self.device)\n",
    "            model_kwargs = dict(data_info={\"img_hw\": hw, \"aspect_ratio\": ar}, mask=emb_masks)\n",
    "\n",
    "            scheduler = DPMS_SDE(\n",
    "                self.model,\n",
    "                condition=caption_embs,\n",
    "                uncondition=null_y,\n",
    "                guidance_type=self.guidance_type,\n",
    "                cfg_scale=guidance_scale,\n",
    "                pag_scale=pag_guidance_scale,\n",
    "                pag_applied_layers=self.config.model.pag_applied_layers,\n",
    "                model_type=\"flow\",\n",
    "                model_kwargs=model_kwargs,\n",
    "                schedule=\"FLOW\",\n",
    "            )\n",
    "            scheduler.register_progress_bar(self.progress_fn)\n",
    "            sample = scheduler.sample(\n",
    "                z,\n",
    "                steps=num_inference_steps,\n",
    "                order=2,\n",
    "                skip_type=\"time_uniform_flow\",\n",
    "                method=\"multistep\",\n",
    "                flow_shift=self.flow_shift,\n",
    "            )\n",
    "\n",
    "        sample = sample.to(self.vae_dtype)\n",
    "        with torch.no_grad():\n",
    "            sample = vae_decode(self.config.vae.vae_type, self.vae, sample)\n",
    "\n",
    "        if use_resolution_binning:\n",
    "            sample = resize_and_crop_tensor(sample, self.ori_width, self.ori_height)\n",
    "        samples.append(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    return samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sana_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
